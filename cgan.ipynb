{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tfe = tf.contrib.eager\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import os, time, itertools, imageio, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image(images):\n",
    "    images = np.reshape(images, [images.shape[0], -1])\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "    \n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]),cmap='gray')\n",
    "    return\n",
    "\n",
    "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
    "    test_images = sess.run(G_sample, {z: fixed_z_, y: fixed_y_})\n",
    "\n",
    "    size_figure_grid = 10\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
    "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "        ax[i, j].get_xaxis().set_visible(False)\n",
    "        ax[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "    for k in range(size_figure_grid*size_figure_grid):\n",
    "        i = k // size_figure_grid\n",
    "        j = k % size_figure_grid\n",
    "        ax[i, j].cla()\n",
    "        ax[i, j].imshow(np.reshape(test_images[k], (28, 28)), cmap='gray')\n",
    "\n",
    "    label = 'Epoch {0}'.format(num_epoch)\n",
    "    fig.text(0.5, 0.04, label, ha='center')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def leaky_relu(x, alpha):\n",
    "    condition = tf.less(x, 0)\n",
    "    return tf.where(condition, alpha * x, x)\n",
    "    \n",
    "def sample_noise(batch_size, dim):\n",
    "    z = np.random.normal(0, 1, (batch_size, dim))\n",
    "    return z\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label preprocess\n",
    "onehot = np.eye(10)\n",
    "temp_z_ = np.random.normal(0, 1, (10, 100))\n",
    "fixed_z_ = temp_z_\n",
    "fixed_y_ = np.zeros((10, 1))\n",
    "\n",
    "for i in range(9):\n",
    "    fixed_z_ = np.concatenate([fixed_z_, temp_z_], 0)\n",
    "    temp = np.ones((10,1)) + i\n",
    "    fixed_y_ = np.concatenate([fixed_y_, temp], 0)\n",
    "\n",
    "fixed_y_ = onehot[fixed_y_.astype(np.int32)].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset\n",
    "class MNIST(object):\n",
    "    def __init__(self, batch_size, shuffle=False):\n",
    "        train, _ = tf.keras.datasets.mnist.load_data()\n",
    "        X, y = train\n",
    "        X = X.astype(np.float32)/255.\n",
    "        X = X.reshape((X.shape[0], -1))\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.range(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generator and discriminator\n",
    "def generator(z, y):\n",
    "    with tf.variable_scope('generator'):\n",
    "#         y1 = tf.layers.dense(y, units=1000, activation=tf.nn.relu)\n",
    "#         z1 = tf.layers.dense(z, units=200, activation=tf.nn.relu)\n",
    "        w_init = tf.contrib.layers.xavier_initializer()\n",
    "        combine = tf.concat([z, y], 1)\n",
    "        dense1 = tf.layers.dense(combine, 128, activation=tf.nn.relu, kernel_initializer=w_init)\n",
    "        #h = tf.keras.layers.dropout(0.5)\n",
    "        output = tf.layers.dense(dense1, 784, activation=tf.nn.tanh, kernel_initializer=w_init)\n",
    "        return output\n",
    "\n",
    "def discriminator(x, y):\n",
    "    with tf.variable_scope('discriminator'):\n",
    "        w_init = tf.contrib.layers.xavier_initializer()\n",
    "        x1 = tf.contrib.layers.maxout(x, 784)\n",
    "        y1 = tf.contrib.layers.maxout(y, 10)\n",
    "        combine = tf.concat([x1, y1], 1)\n",
    "        dense1 = tf.layers.dense(combine, 128, kernel_initializer=w_init)\n",
    "        l_relu = leaky_relu(dense1, alpha=0.2)\n",
    "        output = tf.layers.dense(l_relu, 1, kernel_initializer=w_init)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gan_loss(logits_real, logits_fake):\n",
    "    label_real = tf.ones_like(logits_real)\n",
    "    label_fake = tf.zeros_like(logits_fake)\n",
    "    \n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label_real, logits=logits_real))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label_fake, logits=logits_fake))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    \n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_fake), logits=logits_fake))\n",
    "    return D_loss, G_loss\n",
    "\n",
    "def get_solver(learning_rate=0.0002, beta1=0.5):\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    return D_solver, G_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\THINKPAD\\Anaconda5.3.0\\envs\\cs231n\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\THINKPAD\\Anaconda5.3.0\\envs\\cs231n\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "#put it together\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# number of images for each batch\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "z = tf.placeholder(tf.float32, [None, 100])\n",
    "\n",
    "G_sample = generator(z, y)\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    logits_real = discriminator(x, y)\n",
    "    scope.reuse_variables()\n",
    "    logits_fake = discriminator(G_sample, y)\n",
    "\n",
    "D_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'discriminator')\n",
    "G_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'generator')\n",
    "\n",
    "D_solver, G_solver = get_solver()\n",
    "D_loss, G_loss = gan_loss(logits_real, logits_fake)\n",
    "\n",
    "D_train_step = D_solver.minimize(D_loss, var_list=D_var)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_var)\n",
    "D_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS,'discriminator')\n",
    "G_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS,'generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run a gan\n",
    "def run_a_gan(sess, D_train_step, D_loss, G_train_step, G_loss, D_extra_step,G_extra_step,\\\n",
    "              batch_size=100, num_epoch=100):\n",
    "    #(x_train, y_train), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    x_train = preprocess_img(mnist.train.images)\n",
    "    y_train = mnist.train.labels\n",
    "    for epoch in range(num_epoch):\n",
    "        for iter in range(len(x_train) // batch_size):\n",
    "            #update discriminator\n",
    "            x_ = x_train[iter * batch_size:(iter + 1) * batch_size]\n",
    "            x_ = x_.reshape((x_.shape[0],-1))\n",
    "            y_ = y_train[iter * batch_size:(iter + 1) * batch_size]\n",
    "            z_ = sample_noise(batch_size, 100)\n",
    "            _, D_loss_curr = sess.run([D_train_step, D_loss], feed_dict={x:x_, y:y_, z:z_})\n",
    "            \n",
    "            #update generator\n",
    "            y_ = np.random.randint(0, 9, (batch_size, 1))\n",
    "            y_ = onehot[y_.astype(np.int32)].squeeze()\n",
    "            z_ = sample_noise(batch_size, 100)\n",
    "            _, G_loss_curr = sess.run([G_train_step, G_loss], feed_dict={z:z_, x:x_, y:y_})\n",
    "            \n",
    "        print(\"Epoch:{}, D:{:.4}, G:{:.4}\".format(epoch+1, D_loss_curr, G_loss_curr))\n",
    "        \n",
    "        root = 'My_CGAN_result/'\n",
    "        model = 'maxout1_/'\n",
    "  \n",
    "        PATH = root + model + str(epoch+1) + '.png'\n",
    "        if not os.path.isdir(root):\n",
    "            os.mkdir(root)\n",
    "        if not os.path.isdir(root + model):\n",
    "            os.mkdir(root + model)\n",
    "\n",
    "        show_result(epoch+1, save=True, path=PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:1, D:0.9081, G:0.9884\n",
      "Epoch:2, D:1.146, G:0.9925\n",
      "Epoch:3, D:1.18, G:1.505\n",
      "Epoch:4, D:1.23, G:1.337\n",
      "Epoch:5, D:1.232, G:0.8375\n",
      "Epoch:6, D:1.253, G:0.9206\n",
      "Epoch:7, D:1.404, G:0.8874\n",
      "Epoch:8, D:1.059, G:1.283\n",
      "Epoch:9, D:1.013, G:1.031\n",
      "Epoch:10, D:1.483, G:0.7436\n",
      "Epoch:11, D:1.17, G:0.8256\n",
      "Epoch:12, D:1.32, G:0.6497\n",
      "Epoch:13, D:1.075, G:0.8659\n",
      "Epoch:14, D:0.9432, G:1.351\n",
      "Epoch:15, D:0.7803, G:1.805\n",
      "Epoch:16, D:0.7497, G:1.92\n",
      "Epoch:17, D:0.9555, G:1.367\n",
      "Epoch:18, D:1.084, G:1.01\n",
      "Epoch:19, D:0.9863, G:1.108\n",
      "Epoch:20, D:1.042, G:1.054\n",
      "Epoch:21, D:0.9336, G:1.457\n",
      "Epoch:22, D:1.147, G:1.112\n",
      "Epoch:23, D:1.358, G:0.6721\n",
      "Epoch:24, D:1.043, G:1.039\n",
      "Epoch:25, D:1.096, G:0.8088\n",
      "Epoch:26, D:0.9489, G:1.233\n",
      "Epoch:27, D:1.098, G:1.389\n",
      "Epoch:28, D:0.8794, G:1.209\n",
      "Epoch:29, D:0.8883, G:1.427\n",
      "Epoch:30, D:1.038, G:1.134\n",
      "Epoch:31, D:0.8279, G:1.396\n",
      "Epoch:32, D:0.7783, G:1.288\n",
      "Epoch:33, D:0.6526, G:1.662\n",
      "Epoch:34, D:0.8555, G:1.101\n",
      "Epoch:35, D:1.271, G:0.9031\n",
      "Epoch:36, D:1.22, G:0.6647\n",
      "Epoch:37, D:0.7767, G:1.206\n"
     ]
    }
   ],
   "source": [
    "with get_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_a_gan(sess,D_train_step,D_loss,G_train_step,G_loss,D_extra_step,G_extra_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
